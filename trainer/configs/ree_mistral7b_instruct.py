"""
Config treningowy Ree ‚Äì Mistral-7B-Instruct pod turniej.

To jest CZYSTY CONFIG (bez side-effect√≥w).
Nic siƒô samo nie odpala, dop√≥ki:
- kto≈õ tego nie zaimportuje
- i nie zbuduje z tego TrainerProxyRequest / TrainingData itp.

Zaleta: wszystkie wa≈ºne decyzje (model, dataset, hyperparamy)
sƒÖ w jednym miejscu, w czytelnej formie.
"""

from dataclasses import dataclass, asdict
from typing import Dict, Any


@dataclass
class InstructTaskConfig:
    """
    Og√≥lny config dla zadania typu InstructTextTask.

    Pola sƒÖ dobrane tak, ≈ºeby:
    - da≈Ço siƒô z nich ≈Çatwo zbudowaƒá payload dla trenera / walidatora,
    - by≈Çy czytelne przy tuningu (zmieniasz liczby tutaj, a nie w 10 miejscach).
    """
    # Id taska ‚Äì mo≈ºesz zachowaƒá sp√≥jno≈õƒá z tym, co masz w bashu
    task_id: str

    # Typ zadania ‚Äì w Twoim repo to jest InstructTextTask
    task_type: str

    # Model bazowy
    model_name_or_path: str

    # Dataset (HuggingFace name, S3 path, itd.)
    dataset_name: str
    dataset_split: str

    # Mapowanie p√≥l datasetu na format oczekiwany przez InstructTextTask
    dataset_mapping: Dict[str, str]

    # Format:
    # - "hf"   ‚Üí HuggingFace dataset
    # - "json" ‚Üí lokalny json
    # - "csv"  ‚Üí lokalny csv
    # - "s3"   ‚Üí dane z S3
    file_format: str

    # Ile godzin walidator ma na sko≈Ñczenie taska
    hours_to_complete: int

    # Hyperparamy treningu
    max_steps: int
    max_seq_length: int
    per_device_train_batch_size: int
    gradient_accumulation_steps: int
    learning_rate: float
    warmup_ratio: float
    weight_decay: float
    lr_scheduler_type: str
    bf16: bool
    gradient_checkpointing: bool

    def to_training_payload(self) -> Dict[str, Any]:
        """
        Zwraca s≈Çownik, kt√≥ry mo≈ºna wrzuciƒá w:
        - TrainingData / TrainerProxyRequest
        albo
        - bezpo≈õrednio do jakiego≈õ launchera trenera.

        Przyk≈Çad u≈ºycia (pseudo):

        payload = REE_MISTRAL7B_INSTRUCT.to_training_payload()
        TrainerProxyRequest(training_data=payload, hotkey=...)
        """
        return asdict(self)


# üî• Konkretny config ‚Äûpod maxa‚Äù dla 7B

REE_MISTRAL7B_INSTRUCT = InstructTaskConfig(
    task_id="ree-7b-instruct-tourn-001",
    task_type="InstructTextTask",

    # 7B model bazowy
    model_name_or_path="mistralai/Mistral-7B-Instruct-v0.2",

    # Instruct dataset ‚Äì klasyczny Alpaca
    dataset_name="tatsu-lab/alpaca",
    dataset_split="train",

    # Mapowanie p√≥l:
    # - instruction ‚Üí prompt
    # - output      ‚Üí target
    dataset_mapping={
        "field_instruction": "instruction",
        "field_output": "output",
    },

    file_format="hf",

    # Ile czasu walidator ma na trening
    hours_to_complete=16,

    # Hyperparamy (agresywne, ale nie samob√≥jcze na 7B)
    max_steps=2500,                   # ~2.5k krok√≥w SFT
    max_seq_length=2048,              # standard dla Mistrala
    per_device_train_batch_size=8,    # batch na jednƒÖ kartƒô
    gradient_accumulation_steps=16,   # efektywny batch 8*16 = 128
    learning_rate=1e-5,
    warmup_ratio=0.03,
    weight_decay=0.1,
    lr_scheduler_type="cosine",
    bf16=True,
    gradient_checkpointing=True,
)
